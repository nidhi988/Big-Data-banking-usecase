Leading bank and credit card services provider is looking forward to use hadoop technologies to handle and analyze large amounts of data. Organization wants to use hadoop ecosystem for storage and analysis of large amount of data.

There are three tables:
1.loan_info
2.credit_card_info
3.shares_info


TABLE DETAILS:-

loan_info

loan_id                loan identifier
user_id                user identifier
last_payement_date     date on which payement was made
payement_installation  amount payable in installation
Date_payable           date when payement is made by user


credit_card_info

cc_number              credit card number
user_id                user identifier
maximum_credit         maximum credit allowed by user
outstanding_balance    current outstanding balance by user
due date               due date for credit card pay


shares_info

share_id              share identifier
company_name          name of organization
Gmt_timestamp         timestamp of share price
share_price           price of share



analysis questions
1.finding out list of users who have atleast two loan installments pending.
2.finding list of users who have healthy credit card but outstanding loan account.




sqoop commands to export data from mysql to hdfs

#here password will be protected by saving password in the file

echo -n "user">>sqoop_mysql_password

#creating directory in hdfs loan_info_stg to store the table data.
hadoop fs -mkdir /bank
hadoop fs -mkdir /bank/loan_info_stg

#creating sqoop job to transfer data in  tables

sqoop job --create sqoop_loan_info --import --connect jdbc:mysql://localhost/bank --username user --table loan_info --password-file file:///home/user/sqoop_mysql_password --target-dir /bank/loan_info_stg -m1


sqoop job --exec sqoop_loan_info

hadoop fs -mkdir /bank/credit_card_info_stg

sqoop job --create sqoop_credit_card_info --import --connect jdbc:mysql://localhost/bank --username user --table credit_card_info --password-file file:///home/user/sqoop_mysql_password --target-dir /bank/credit_card_info_stg -m1

sqoop job --exec sqoop_credit_card_info


hadoop fs -mkdir /bank/shares_info_stg

sqoop job --create sqoop_shares_info --import --connect jdbc:mysql://localhost/bank --username user --table shares_info --password-file file:///home/user/sqoop_mysql_password --target-dir /bank/shares_info_stg -m1

sqoop job --exec sqoop_shares_info


#to check all sqoop job lists
sqoop job --list

